{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "\n",
    "- A simple for loop was used to determine the two best models introduced below. This was done by identifying the best candidates with default hyperparameters from a list of 11 potential candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from   sklearn.decomposition   import PCA\n",
    "from   sklearn.model_selection import *\n",
    "from   sklearn.model_selection import train_test_split\n",
    "from   sklearn.pipeline        import Pipeline\n",
    "from   sklearn.preprocessing   import *\n",
    "from   sklearn.impute          import *\n",
    "from   sklearn.linear_model    import *\n",
    "from   sklearn.compose         import *\n",
    "from   sklearn.ensemble        import *\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Metrics\n",
    "from   sklearn.metrics         import mean_absolute_error\n",
    "def relative_absolute_error(test, pred):\n",
    "    avg_test = [np.mean(test)] * len(test)\n",
    "    return np.sum(np.abs(pred-test))/np.sum(np.abs(avg_test-test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in and split the data set\n",
    "\n",
    "- Because of the limited number of samples in the dataset, I opted to reduce the train/test ratio to 0.9:0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"country_stats_mental_health.csv\").set_index('Country')\n",
    "X = df.drop('Suicide Rate 2016', axis=1)\n",
    "y = df['Suicide Rate 2016']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model\n",
    "\n",
    "- Create a baseline regression model to compare with engineered models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 8.816\n",
      "Relative Absolute Error: 3.009\n"
     ]
    }
   ],
   "source": [
    "categorical_columns = ['Region']\n",
    "numerical_columns = (X_train.dtypes == float)\n",
    "\n",
    "con_pipe = Pipeline([('imputer', SimpleImputer(add_indicator=True)),\n",
    "                    ])\n",
    "cat_pipe = Pipeline([('imputer', SimpleImputer(strategy=\"most_frequent\")),\n",
    "                     ('ohe', OneHotEncoder(handle_unknown='ignore'))\n",
    "                    ])\n",
    "\n",
    "preprocessing = ColumnTransformer([('categorical', cat_pipe, categorical_columns),\n",
    "                                   ('continuous', con_pipe, numerical_columns)])\n",
    "\n",
    "pipe = Pipeline([('preprocessing', preprocessing),\n",
    "                 ('baseline', LinearRegression())])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "y_pred_base = pipe.predict(X_test)\n",
    "\n",
    "print(f\"Mean Absolute Error: {mean_absolute_error(y_test, y_pred_base):.3f}\")\n",
    "print(f\"Relative Absolute Error: {relative_absolute_error(y_test, y_pred_base):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1: Bayesian Ridge Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outline of pipeline creation\n",
    "\n",
    "# Feature Engineering\n",
    "categorical_columns = ['Region']\n",
    "numerical_columns = (X_train.dtypes == float)\n",
    "\n",
    "con_pipe = Pipeline([('imputer', SimpleImputer(add_indicator=True)),\n",
    "                     ('quantile', QuantileTransformer(output_distribution='normal')),\n",
    "                     ('pca', PCA())\n",
    "                    ])\n",
    "cat_pipe = Pipeline([('imputer', SimpleImputer(strategy=\"most_frequent\")),\n",
    "                     ('ohe', OneHotEncoder(handle_unknown='ignore'))\n",
    "                    ])\n",
    "\n",
    "preprocessing = ColumnTransformer([('categorical', cat_pipe, categorical_columns),\n",
    "                                   ('continuous', con_pipe, numerical_columns)])\n",
    "\n",
    "\n",
    "# Creating the pipeline with Bayesian Ridge\n",
    "pipe = Pipeline([('preprocessing', preprocessing),\n",
    "                 ('bayesianridge', BayesianRidge(normalize=False))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters to cross validate\n",
    "hyperparameters = dict(preprocessing__continuous__imputer__strategy     = ['mean', 'median'],\n",
    "                       preprocessing__continuous__pca__n_components     = [5, 10, 15, 20, 25, 30, 40, 50, 60],\n",
    "                       preprocessing__continuous__quantile__n_quantiles = [10, 20, 30, 40, 50, 100],\n",
    "                       bayesianridge__n_iter                            = [300, 400, 500, 600],\n",
    "                       bayesianridge__alpha_1                           = [10e-6, 10e-5, 10e-4],\n",
    "                       bayesianridge__alpha_2                           = [10e-6, 10e-5, 10e-4],\n",
    "                       bayesianridge__lambda_1                          = [10e-6, 10e-5, 10e-4],\n",
    "                       bayesianridge__lambda_2                          = [10e-6, 10e-5, 10e-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomized search for best parameters (chosen for time efficiency)\n",
    "\n",
    "cv_rand_br = RandomizedSearchCV(estimator=pipe,\n",
    "                                 param_distributions=hyperparameters,\n",
    "                                 n_iter=50,\n",
    "                                 cv=KFold(n_splits=5),\n",
    "                                 n_jobs=-1,\n",
    "                                 verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    }
   ],
   "source": [
    "# Find the best model\n",
    "best_model_br = cv_rand_br.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'preprocessing__continuous__quantile__n_quantiles': 10,\n",
       " 'preprocessing__continuous__pca__n_components': 60,\n",
       " 'preprocessing__continuous__imputer__strategy': 'median',\n",
       " 'bayesianridge__n_iter': 400,\n",
       " 'bayesianridge__lambda_2': 0.0001,\n",
       " 'bayesianridge__lambda_1': 0.001,\n",
       " 'bayesianridge__alpha_2': 1e-05,\n",
       " 'bayesianridge__alpha_1': 0.001}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparameters of best model\n",
    "best_model_br.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 Evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction using the above hyperparameters\n",
    "y_pred_br = best_model_br.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 3.790\n",
      "Relative Absolute Error: 1.294\n"
     ]
    }
   ],
   "source": [
    "# Model evaluation with via two metrics\n",
    "print(f\"Mean Absolute Error: {mean_absolute_error(y_test, y_pred_br):.3f}\")\n",
    "print(f\"Relative Absolute Error: {relative_absolute_error(y_test, y_pred_br):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Outline of pipeline creation\n",
    "\n",
    "# Feature Engineering\n",
    "categorical_columns = ['Region']\n",
    "numerical_columns = (X_train.dtypes == float)\n",
    "\n",
    "con_pipe = Pipeline([('imputer', SimpleImputer(add_indicator=True)),\n",
    "                     ('pca', PCA())\n",
    "                    ])\n",
    "cat_pipe = Pipeline([('imputer', SimpleImputer(strategy=\"most_frequent\")),\n",
    "                     ('ohe', OneHotEncoder(handle_unknown='ignore'))\n",
    "                    ])\n",
    "\n",
    "preprocessing = ColumnTransformer([('categorical', cat_pipe, categorical_columns),\n",
    "                                   ('continuous', con_pipe, numerical_columns)])\n",
    "\n",
    "# Creating the pipeline with Random Forest Regressor\n",
    "pipe = Pipeline([('preprocessing', preprocessing),\n",
    "                 ('randomforest', RandomForestRegressor(n_jobs=-1))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define hyperparameters to cross validate\n",
    "hyperparameters = dict(preprocessing__continuous__imputer__strategy     = ['mean', 'median'],\n",
    "                       preprocessing__continuous__pca__n_components     = [5, 10, 15, 20, 25, 30, 40, 50, 60],\n",
    "                       randomforest__n_estimators                       = [10, 50, 100, 200, 300, 500],\n",
    "                       randomforest__min_samples_split                  = [1, 2, 3, 4],\n",
    "                       randomforest__min_samples_leaf                   = [1, 2, 3, 5, 10, 15],\n",
    "                       randomforest__max_features                       = ['auto', 'sqrt', 'log2'],\n",
    "                       randomforest__criterion                          = [\"mse\", \"mae\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomized search for best parameters (chosen for time efficiency)\n",
    "cv_random_rf = RandomizedSearchCV(estimator=pipe,\n",
    "                                 param_distributions=hyperparameters,\n",
    "                                 n_iter=50,\n",
    "                                 cv=KFold(n_splits=5),\n",
    "                                 n_jobs=-1,\n",
    "                                 verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    }
   ],
   "source": [
    "# Find the best model\n",
    "best_model_rf = cv_random_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'randomforest__n_estimators': 50,\n",
       " 'randomforest__min_samples_split': 4,\n",
       " 'randomforest__min_samples_leaf': 2,\n",
       " 'randomforest__max_features': 'log2',\n",
       " 'randomforest__criterion': 'mae',\n",
       " 'preprocessing__continuous__pca__n_components': 40,\n",
       " 'preprocessing__continuous__imputer__strategy': 'median'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparameters of best model\n",
    "best_model_rf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2 Evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prediction using the above hyperparameters\n",
    "y_pred_rf = best_model_rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 3.559\n",
      "Relative Absolute Error: 1.215\n"
     ]
    }
   ],
   "source": [
    "# Model evaluation with via two metrics\n",
    "print(f\"Mean Absolute Error: {mean_absolute_error(y_test, y_pred_rf):.3f}\")\n",
    "print(f\"Relative Absolute Error: {relative_absolute_error(y_test, y_pred_rf):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model: Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final pipeline, see below for final model parameteres\n",
    "categorical_columns = ['Region']\n",
    "numerical_columns = (X_train.dtypes == float)\n",
    "\n",
    "con_pipe = Pipeline([('imputer', SimpleImputer(add_indicator=True,\n",
    "                                               strategy='median')),\n",
    "                     ('pca', PCA(n_components=40))\n",
    "                    ])\n",
    "cat_pipe = Pipeline([('imputer', SimpleImputer(strategy=\"most_frequent\")),\n",
    "                     ('ohe', OneHotEncoder(handle_unknown='ignore'))\n",
    "                    ])\n",
    "\n",
    "preprocessing = ColumnTransformer([('categorical', cat_pipe, categorical_columns),\n",
    "                                   ('continuous', con_pipe, numerical_columns)])\n",
    "\n",
    "# Creating the pipeline with Random Forest Regressor\n",
    "pipe = Pipeline([('preprocessing', preprocessing),\n",
    "                 ('randomforest', RandomForestRegressor(n_jobs=-1,\n",
    "                                                        n_estimators=50,\n",
    "                                                        min_samples_split=4,\n",
    "                                                        min_samples_leaf=2,\n",
    "                                                        max_features='log2',\n",
    "                                                        criterion='mae'))\n",
    "                ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model HyperParameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'ccp_alpha': 0.0,\n",
       " 'criterion': 'mae',\n",
       " 'max_depth': None,\n",
       " 'max_features': 'log2',\n",
       " 'max_leaf_nodes': None,\n",
       " 'max_samples': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_impurity_split': None,\n",
       " 'min_samples_leaf': 2,\n",
       " 'min_samples_split': 4,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 50,\n",
       " 'n_jobs': -1,\n",
       " 'oob_score': False,\n",
       " 'random_state': None,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe['randomforest'].get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best non-default hyperparameters:\n",
    "\n",
    "- PCA: \n",
    "    - n_components = 40\n",
    "\n",
    "\n",
    "- SimpleImputer:\n",
    "    - strategy = 'median'\n",
    "\n",
    "\n",
    "- Random Forest Regressor:\n",
    "    - n_estimators = 50\n",
    "    - min_samples_split = 4\n",
    "    - min_samples_leaf = 2\n",
    "    - max_features = 'log2'\n",
    "    - criterion = 'mae'\n",
    "    \n",
    "\n",
    "### Parameter Interpretation:\n",
    "\n",
    "In this model, several aspects were adjusted to create the best final version. The first two were part of the feature engineering for continuous features. Cross validation deemed that imputing missing values with the median of the corresponding feature column was most effective. As for PCA, the number of transformed composite components reduced the number of columns from 66 to 40.\n",
    "\n",
    "The random forest regressor had the most tweaks to the default hyperparameters. The decision was made to create a random forest out of 50 decision trees, where the minimum number of samples required to split an internal node was 4. In addition, there was a minimum of 2 samples required to be considered a \"leaf\" node. The maximum features randomly chosen to consider when looking for the best split was log_2(n), where n is the total number of features. Finally, the criteria used to measure the quality of a split was Mean Average Error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In conclusion, the Random Forest Regressor proved to be the best model in predicting suicide rates when given the 66 compiled features from the \"Mental Health and Suicide Rates\" dataset and the \"UN Data Country Profiles\" dataset. For the run shown in the notebook, the Mean Absolute Error was 3.56, implying that the average prediction for suicide rates for any given country was 3.56 percentage values different than the true value. The Relative Absolute Error was 1.215, meaning that the average absolute error was 1.215x larger than that of suicide rate noise's deviation from the average. While this implies that predicting a country's suicide rate from the average of all countries is still a better prediction method, the random forest regressor model performs much better than the baseline model when comparing metrics.\n",
    "\n",
    "For this project, many aspects of the pipeline changed as the module progressed. I became aware of much more sophisticated methods of hyperparameter tuning (random cross-validation) which cut out the loops I was using with a primitive grid search. I learned how to include parts of preprocessing into the pipeline, as well as the syntax to analyze the parameters of both the pipeline itself and the cross_validated models. \n",
    "\n",
    "While I was hoping for a more accurate model, in the end I was satisfied at just how much better my engineered machine learning models performed compared to my baseline model. The inherent randomness of the random forest regressor made it difficult to determine whether it was the best, but I felt that it most consistently performed the best when dealing with different splits in the full data frame. I found out quickly that the quantile transformer was inconsequential in the outcome of the random forest regressor, as the method of splitting doesn't compare the scales of each feature space and therefore rescaling doesn't affect the decision. \n",
    "\n",
    "The question of \"why does this matter?\" only comes up when we analyze the feature importance found during the model construction. The top ten predictive features all had to do with the way government allocates funds for mental health, education, and infrastructure. Features with \"quick fix\" mindsets tended to have less importance in the evaluation, instead favoring long term solutions. While it's difficult to say with complete authority, this model does reinforce the importance of education, infrastructure, and mental healthcare accessibility in managing suicide rates within any given country. While it's incredibly difficult to model human mental health in a quantitative way, this project further reinforced my belief that the right to \n",
    "\n",
    "Next steps to further improve this pipeline includes introducing deep learning and gradient boosting models, ensembling methods, creating more features from those which already exist in the dataset, and upsampling existing data/finding more data which matches the schema of the current features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

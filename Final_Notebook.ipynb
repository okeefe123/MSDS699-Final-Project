{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/okeefe123/MSDS699-Final-Project/blob/main/Final_Notebook.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "*Note: If above banner is not clickable, right click and follow link to access colab notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from   sklearn.decomposition   import PCA\n",
    "from   sklearn.model_selection import *\n",
    "from   sklearn.model_selection import train_test_split\n",
    "from   sklearn.pipeline        import Pipeline\n",
    "from   sklearn.preprocessing   import *\n",
    "from   sklearn.impute          import *\n",
    "from   sklearn.linear_model    import *\n",
    "from   sklearn.compose         import *\n",
    "from   sklearn.ensemble        import *\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics:\n",
    "\n",
    "- For this project, I decided to use two metrics:\n",
    "\n",
    "    1. Mean Absolute Error (MAE): I feel like this metric gives a more interpretable score than RMSE, and a bonus is that it tends to penalize outliers less heavily, which may be more helpful for the small dataset I'm using.\n",
    "    2. Relative Absolute Error (RAE): While MAE works, it's hard to find context for it at a glance. RAE, on the other hand, scales is against the deviation of the the test set's output values from its average. In the context of my project, this intuition of comparing my model's predictions to worldwide average suicide rates as a benchmark makes it a very intuitive way to see the model's effectiveness at a glance. RAE lower than 1, this model will show better predictions which take into consideration the individual country's attributes rather than a vast generalization of several country suicide rates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from   sklearn.metrics         import mean_absolute_error\n",
    "\n",
    "def relative_absolute_error(test, pred):\n",
    "    avg_test = [np.mean(test)] * len(test)\n",
    "    return np.sum(np.abs(pred-test))/np.sum(np.abs(avg_test-test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in and split the data set\n",
    "\n",
    "- Because of the small size of the dataset (100 columns representing 100 countries), I decided to split only into train/test and use cross-validation to assess a \"success\" metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"country_stats_mental_health.csv\").set_index('Country')\n",
    "X = df.drop('Suicide Rate 2016', axis=1)\n",
    "y = df['Suicide Rate 2016']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model\n",
    "\n",
    "- Create a baseline regression model to compare with engineered models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 8.816\n",
      "Relative Absolute Error: 3.009\n"
     ]
    }
   ],
   "source": [
    "categorical_columns = ['Region']\n",
    "numerical_columns = (X_train.dtypes == float)\n",
    "\n",
    "con_pipe = Pipeline([('imputer', SimpleImputer(add_indicator=True)),\n",
    "                    ])\n",
    "cat_pipe = Pipeline([('imputer', SimpleImputer(strategy=\"most_frequent\")),\n",
    "                     ('ohe', OneHotEncoder(handle_unknown='ignore'))\n",
    "                    ])\n",
    "\n",
    "preprocessing = ColumnTransformer([('categorical', cat_pipe, categorical_columns),\n",
    "                                   ('continuous', con_pipe, numerical_columns)])\n",
    "\n",
    "pipe = Pipeline([('preprocessing', preprocessing),\n",
    "                 ('baseline', LinearRegression())])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "y_pred_base = pipe.predict(X_test)\n",
    "\n",
    "print(f\"Mean Absolute Error: {mean_absolute_error(y_test, y_pred_base):.3f}\")\n",
    "print(f\"Relative Absolute Error: {relative_absolute_error(y_test, y_pred_base):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First glance tells me there's nowhere to go but up in terms of metrics. The mean absolute error was on the same scale as many country's rates, and the relative absolute error suggests that this performs worse than noise in the true suicide rate values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Default Model Selection:\n",
    "- A simple for loop was used to determine the two best models introduced below. This was done by identifying the best candidates with default hyperparameters from a list of 11 potential candidates. The candidates and their Mean Absolute Error scores were (reporting from another notebook):\n",
    "\n",
    "    1. BayesianRidge():$\\hspace{23mm}$ 3.696\n",
    "    2. RandomForestRegressor():$\\hspace{5mm}$     3.712\n",
    "    3. ElasticNet(): $\\hspace{29mm}$                3.855\n",
    "    4. Lasso():$\\hspace{38mm}$ 3.918\n",
    "    5. LassoLars():$\\hspace{31mm}$ 4.196\n",
    "    6. Ridge():$\\hspace{38mm}$ 6.403\n",
    "    7. RANSACRegressor():$\\hspace{16mm}$ 6.422\n",
    "    8. SGDRegressor():$\\hspace{23mm}$ 7.238\n",
    "    9. LinearRegression(): $\\hspace{17mm}$ 7.986\n",
    "    10. TheilSenRegressor(): $\\hspace{15mm}$ 8.725\n",
    "    11. HuberRegressor(): $\\hspace{19mm}$ 9.238\n",
    "   \n",
    "$$ $$\n",
    "\n",
    "- As can be seen from the list, the appropriate choice of model alone vastly improved the Mean Absolute Error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1: Bayesian Ridge Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outline of pipeline creation\n",
    "\n",
    "# Feature Engineering\n",
    "categorical_columns = ['Region']\n",
    "numerical_columns = (X_train.dtypes == float)\n",
    "\n",
    "con_pipe = Pipeline([('imputer', SimpleImputer(add_indicator=True)),\n",
    "                     ('quantile', QuantileTransformer(output_distribution='normal')),\n",
    "                     ('pca', PCA())\n",
    "                    ])\n",
    "cat_pipe = Pipeline([('imputer', SimpleImputer(strategy=\"most_frequent\")),\n",
    "                     ('ohe', OneHotEncoder(handle_unknown='ignore'))\n",
    "                    ])\n",
    "\n",
    "preprocessing = ColumnTransformer([('categorical', cat_pipe, categorical_columns),\n",
    "                                   ('continuous', con_pipe, numerical_columns)])\n",
    "\n",
    "\n",
    "# Creating the pipeline with Bayesian Ridge\n",
    "pipe = Pipeline([('preprocessing', preprocessing),\n",
    "                 ('bayesianridge', BayesianRidge(normalize=False))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation\n",
    "\n",
    "- While the imputer hyperparameter was just to check most useful strategy, the other two were more strategic:\n",
    "    1. PCA: The number of components can only be as large as the number of features, so I decided on multiples of five up to 30, then multiples of 10 from there to the number of features in the data. I expected the number of components to be on the larger side (every feature counts!), and was proven correct.\n",
    "    2. Quantiles: The number of quantiles is again constrained by the data set. Any larger than 100 will not give any more accurate of a prediction. I predict that each quantile should ideally be given 1-2 points. I wsa actually incorrect about this, with the best parameter being 10 quantiles (i.e. 10 samples per quantile)\n",
    "    \n",
    "- From past experience, the number of iterations was by far the most helpful in improving metric scores for these regularized linear regression models. The alpha/lambda hyperparameters also have significance in the regularization approximations, so I thought that it would be useful to test multiple combinations to see how the model reacted. The one trend I see is that the model prefered higher values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters to cross validate\n",
    "hyperparameters = dict(preprocessing__continuous__imputer__strategy     = ['mean', 'median'],\n",
    "                       preprocessing__continuous__pca__n_components     = [5, 10, 15, 20, 25, 30, 40, 50, 60],\n",
    "                       preprocessing__continuous__quantile__n_quantiles = [10, 20, 30, 40, 50, 100],\n",
    "                       bayesianridge__n_iter                            = [300, 400, 500, 600],\n",
    "                       bayesianridge__alpha_1                           = [10e-6, 10e-5, 10e-4, 10e-3],\n",
    "                       bayesianridge__alpha_2                           = [10e-6, 10e-5, 10e-4, 10e-3],\n",
    "                       bayesianridge__lambda_1                          = [10e-6, 10e-5, 10e-4, 10e-3],\n",
    "                       bayesianridge__lambda_2                          = [10e-6, 10e-5, 10e-4, 10e-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomized search for best parameters (chosen for time efficiency)\n",
    "\n",
    "cv_rand_br = RandomizedSearchCV(estimator=pipe,\n",
    "                                 param_distributions=hyperparameters,\n",
    "                                 n_iter=50,\n",
    "                                 cv=KFold(n_splits=5),\n",
    "                                 n_jobs=-1,\n",
    "                                 verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    }
   ],
   "source": [
    "# Find the best model\n",
    "best_model_br = cv_rand_br.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'preprocessing__continuous__quantile__n_quantiles': 10,\n",
       " 'preprocessing__continuous__pca__n_components': 60,\n",
       " 'preprocessing__continuous__imputer__strategy': 'median',\n",
       " 'bayesianridge__n_iter': 400,\n",
       " 'bayesianridge__lambda_2': 0.0001,\n",
       " 'bayesianridge__lambda_1': 0.001,\n",
       " 'bayesianridge__alpha_2': 1e-05,\n",
       " 'bayesianridge__alpha_1': 0.001}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparameters of best model\n",
    "best_model_br.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 Evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction using the above hyperparameters\n",
    "y_pred_br = best_model_br.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 3.790\n",
      "Relative Absolute Error: 1.294\n"
     ]
    }
   ],
   "source": [
    "# Model evaluation with via two metrics\n",
    "print(f\"Mean Absolute Error: {mean_absolute_error(y_test, y_pred_br):.3f}\")\n",
    "print(f\"Relative Absolute Error: {relative_absolute_error(y_test, y_pred_br):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is interesting to me, as the default BayesianRidge() I ran in model selection actually performed better than the engineered model above. I must have gotten lucky with the original train/test split! Regardless, it produced much better results than the base model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Outline of pipeline creation\n",
    "\n",
    "# Feature Engineering\n",
    "categorical_columns = ['Region']\n",
    "numerical_columns = (X_train.dtypes == float)\n",
    "\n",
    "con_pipe = Pipeline([('imputer', SimpleImputer(add_indicator=True)),\n",
    "                     ('pca', PCA())\n",
    "                    ])\n",
    "cat_pipe = Pipeline([('imputer', SimpleImputer(strategy=\"most_frequent\")),\n",
    "                     ('ohe', OneHotEncoder(handle_unknown='ignore'))\n",
    "                    ])\n",
    "\n",
    "preprocessing = ColumnTransformer([('categorical', cat_pipe, categorical_columns),\n",
    "                                   ('continuous', con_pipe, numerical_columns)])\n",
    "\n",
    "# Creating the pipeline with Random Forest Regressor\n",
    "pipe = Pipeline([('preprocessing', preprocessing),\n",
    "                 ('randomforest', RandomForestRegressor(n_jobs=-1))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation\n",
    "\n",
    "- I again decided to test both imputer strategies to get a feel for the best way to fill in missing values, and PCA components also have the same logic as before, though it was interesting to see that this model preferred less features than BayesianRidge(). I decided against a scale transformation, as scale doesn't impact Random Forest models (i.e. the loss for a feature split is inherently normalized via the ratio in MSE/MAE), so quantile transformation was tossed from preprocessing.\n",
    "\n",
    "- As for the model hyperparameters, I took some hints from Terence's lectures in optimizing the trees. \n",
    "    1. The number of decision trees (n_estimators) tends to have diminishing returns after a certain amount. In the case of the best model, this value was 50.\n",
    "    2. Max_features seemed to be the next best hyperparameter to tweak. This establishes how many features are considered for each decision node split, and so can have a big impact on the ultimate random forest aggregate.\n",
    "    3. Criterion determines how much we would like to penalize outliers during loss calculation. Given the number of features and the limited samples, I hypothesized that mean abolute error would win just because data is so entrenched in the human experience (which can be all over the place). My hypothesis was verified by the best model.\n",
    "    4. Min samples per split and min samples per leaf are both restrictions which can alter the final state of individual decision trees, so I included it in cross-validation to cover all bases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define hyperparameters to cross validate\n",
    "hyperparameters = dict(preprocessing__continuous__imputer__strategy     = ['mean', 'median'],\n",
    "                       preprocessing__continuous__pca__n_components     = [5, 10, 15, 20, 25, 30, 40, 50, 60],\n",
    "                       randomforest__n_estimators                       = [10, 50, 100, 200, 300, 500],\n",
    "                       randomforest__min_samples_split                  = [1, 2, 3, 4],\n",
    "                       randomforest__min_samples_leaf                   = [1, 2, 3, 5, 10, 15],\n",
    "                       randomforest__max_features                       = ['auto', 'sqrt', 'log2'],\n",
    "                       randomforest__criterion                          = [\"mse\", \"mae\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomized search for best parameters (chosen for time efficiency)\n",
    "cv_random_rf = RandomizedSearchCV(estimator=pipe,\n",
    "                                 param_distributions=hyperparameters,\n",
    "                                 n_iter=50,\n",
    "                                 cv=KFold(n_splits=5),\n",
    "                                 n_jobs=-1,\n",
    "                                 verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    }
   ],
   "source": [
    "# Find the best model\n",
    "best_model_rf = cv_random_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'randomforest__n_estimators': 50,\n",
       " 'randomforest__min_samples_split': 4,\n",
       " 'randomforest__min_samples_leaf': 2,\n",
       " 'randomforest__max_features': 'log2',\n",
       " 'randomforest__criterion': 'mae',\n",
       " 'preprocessing__continuous__pca__n_components': 40,\n",
       " 'preprocessing__continuous__imputer__strategy': 'median'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparameters of best model\n",
    "best_model_rf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2 Evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prediction using the above hyperparameters\n",
    "y_pred_rf = best_model_rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 3.559\n",
      "Relative Absolute Error: 1.215\n"
     ]
    }
   ],
   "source": [
    "# Model evaluation with via two metrics\n",
    "print(f\"Mean Absolute Error: {mean_absolute_error(y_test, y_pred_rf):.3f}\")\n",
    "print(f\"Relative Absolute Error: {relative_absolute_error(y_test, y_pred_rf):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This model performed the best, though in ths specific case the relative absolute error suggests we'd have better luck if using the average of known suicide rates. Still, it performed marginally better than the lucky default model run and much much more accurate than the baseline model outlined above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model: Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final pipeline, see below for final model parameteres\n",
    "categorical_columns = ['Region']\n",
    "numerical_columns = (X_train.dtypes == float)\n",
    "\n",
    "con_pipe = Pipeline([('imputer', SimpleImputer(add_indicator=True,\n",
    "                                               strategy='median')),\n",
    "                     ('pca', PCA(n_components=40))\n",
    "                    ])\n",
    "cat_pipe = Pipeline([('imputer', SimpleImputer(strategy=\"most_frequent\")),\n",
    "                     ('ohe', OneHotEncoder(handle_unknown='ignore'))\n",
    "                    ])\n",
    "\n",
    "preprocessing = ColumnTransformer([('categorical', cat_pipe, categorical_columns),\n",
    "                                   ('continuous', con_pipe, numerical_columns)])\n",
    "\n",
    "# Creating the pipeline with Random Forest Regressor\n",
    "pipe = Pipeline([('preprocessing', preprocessing),\n",
    "                 ('randomforest', RandomForestRegressor(n_jobs=-1,\n",
    "                                                        n_estimators=50,\n",
    "                                                        min_samples_split=4,\n",
    "                                                        min_samples_leaf=2,\n",
    "                                                        max_features='log2',\n",
    "                                                        criterion='mae'))\n",
    "                ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model HyperParameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'ccp_alpha': 0.0,\n",
       " 'criterion': 'mae',\n",
       " 'max_depth': None,\n",
       " 'max_features': 'log2',\n",
       " 'max_leaf_nodes': None,\n",
       " 'max_samples': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_impurity_split': None,\n",
       " 'min_samples_leaf': 2,\n",
       " 'min_samples_split': 4,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 50,\n",
       " 'n_jobs': -1,\n",
       " 'oob_score': False,\n",
       " 'random_state': None,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe['randomforest'].get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best non-default hyperparameters:\n",
    "\n",
    "- PCA: \n",
    "    - n_components = 40\n",
    "\n",
    "\n",
    "- SimpleImputer:\n",
    "    - strategy = 'median'\n",
    "\n",
    "\n",
    "- Random Forest Regressor:\n",
    "    - n_estimators = 50\n",
    "    - min_samples_split = 4\n",
    "    - min_samples_leaf = 2\n",
    "    - max_features = 'log2'\n",
    "    - criterion = 'mae'\n",
    "    \n",
    "\n",
    "### Parameter Interpretation:\n",
    "\n",
    "In this model, several aspects were adjusted to create the best final version. The first two were part of the feature engineering for continuous features. Cross validation deemed that imputing missing values with the median of the corresponding feature column was most effective. As for PCA, the number of transformed composite components reduced the number of columns from 66 to 40.\n",
    "\n",
    "The random forest regressor had the most tweaks to the default hyperparameters. The decision was made to create a random forest out of 50 decision trees, where the minimum number of samples required to split an internal node was 4. In addition, there was a minimum of 2 samples required to be considered a \"leaf\" node. The maximum features randomly chosen to consider when looking for the best split was log_2(n), where n is the total number of features. Finally, the criteria used to measure the quality of a split was Mean Average Error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In conclusion, the Random Forest Regressor proved to be the best model in predicting suicide rates when given the 66 compiled features from the \"Mental Health and Suicide Rates\" dataset and the \"UN Data Country Profiles\" dataset. For the run shown in the notebook, the Mean Absolute Error was 3.56, implying that the average prediction for suicide rates for any given country was 3.56 percentage values different than the true value. The Relative Absolute Error was 1.215, meaning that the average absolute error was 1.215x larger than that of suicide rate noise's deviation from the average. While this implies that predicting a country's suicide rate from the average of all countries is still a better prediction method, the random forest regressor model performs much better than the baseline model when comparing metrics.\n",
    "\n",
    "For this project, many aspects of the pipeline changed as the module progressed. I became aware of much more sophisticated methods of hyperparameter tuning (random cross-validation) which cut out the loops I was using with a primitive grid search. I learned how to include parts of preprocessing into the pipeline, as well as the syntax to analyze the parameters of both the pipeline itself and the cross_validated models. \n",
    "\n",
    "While I was hoping for a more accurate model, in the end I was satisfied at just how much better my engineered machine learning models performed compared to my baseline model. The inherent randomness of the random forest regressor made it difficult to determine whether it was the best, but I felt that it most consistently performed the best when dealing with different splits in the full data frame. I found out quickly that the quantile transformer was inconsequential in the outcome of the random forest regressor, as the method of splitting doesn't compare the scales of each feature space and therefore rescaling doesn't affect the decision. \n",
    "\n",
    "The question of \"why does this matter?\" only comes up when we analyze the feature importance found during the model construction. The top ten predictive features all had to do with the way government allocates funds for mental health, education, and infrastructure. Features with \"quick fix\" mindsets tended to have less importance in the evaluation, instead favoring long term solutions. While it's difficult to say with complete authority, this model does reinforce the importance of education, infrastructure, and mental healthcare accessibility in managing suicide rates within any given country. While it's incredibly difficult to model human mental health in a quantitative way, this project further reinforced my belief that the right to \n",
    "\n",
    "Next steps to further improve this pipeline includes introducing deep learning and gradient boosting models, ensembling methods, creating more features from those which already exist in the dataset, and upsampling existing data/finding more data which matches the schema of the current features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(yes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

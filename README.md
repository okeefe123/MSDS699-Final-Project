Summary:

In conclusion, the Random Forest Regressor proved to be the best model in predicting suicide rates when given the 66 compiled features from the "Mental Health and Suicide Rates" dataset and the "UN Data Country Profiles" dataset. For the run shown in the notebook, the Mean Absolute Error was 3.56, implying that the average prediction for suicide rates for any given country was 3.56 percentage values different than the true value. The Relative Absolute Error was 1.215, meaning that the average absolute error was 1.215x larger than that of suicide rate noise's deviation from the average. While this implies that predicting a country's suicide rate from the average of all countries is still a better prediction method, the random forest regressor model performs much better than the baseline model when comparing metrics.

For this project, many aspects of the pipeline changed as the module progressed. I became aware of much more sophisticated methods of hyperparameter tuning (random cross-validation) which cut out the loops I was using with a primitive grid search. I learned how to include parts of preprocessing into the pipeline, as well as the syntax to analyze the parameters of both the pipeline itself and the cross_validated models. 

While I was hoping for a more accurate model, in the end I was satisfied at just how much better my engineered machine learning models performed compared to my baseline model. The inherent randomness of the random forest regressor made it difficult to determine whether it was the best, but I felt that it most consistently performed the best when dealing with different splits in the full data frame. I found out quickly that the quantile transformer was inconsequential in the outcome of the random forest regressor, as the method of splitting doesn't compare the scales of each feature space and therefore rescaling doesn't affect the decision. 

The question of "why does this matter?" only comes up when we analyze the feature importance found during the model construction. The top ten predictive features all had to do with the way government allocates funds for mental health, education, and infrastructure. Features with "quick fix" mindsets tended to have less importance in the evaluation, instead favoring long term solutions. While it's difficult to say with complete authority, this model does reinforce the importance of education, infrastructure, and mental healthcare accessibility in managing suicide rates within any given country. While it's incredibly difficult to model human mental health in a quantitative way, this project further reinforced my belief that the right to live is not fully realized unless a government provides its citizens with a sufficient and accessible safety net.

Next steps to further improve this pipeline includes introducing deep learning and gradient boosting models, ensembling methods, creating more features from those which already exist in the dataset, and upsampling existing data/finding more data which matches the schema of the current features.
